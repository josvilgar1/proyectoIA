{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as random\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializeRMatrix(numRows, numCols):\n",
    "    return np.array([-1 for _ in range((numRows**2)*(numCols**2))]).reshape(numRows**2, numCols**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "¿Cuál es el tamaño del problema?  16\n"
     ]
    }
   ],
   "source": [
    "size = int(input('¿Cuál es el tamaño del problema? '))\n",
    "num_col = size\n",
    "num_fil = size\n",
    "states = (num_col*num_fil)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "¿Cuál es el estado objetivo? 8\n"
     ]
    }
   ],
   "source": [
    "objetivo = int(input('¿Cuál es el estado objetivo?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo del tablero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos la Matriz tablero primero. Para ello, según hemos observado en el ejemplo, lo valores del tablero se incrementan por lo que hemos denominado __capas__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    -Capa 0: \n",
    "            [0]\n",
    "    -Capa 1: \n",
    "            [0  3]\n",
    "            [1  2]\n",
    "    -Capa 2:\n",
    "            [0  3  8]\n",
    "            [1  2  7]\n",
    "            [4  5  6]\n",
    "    -Capa 3: \n",
    "            [ 0   3   8  15]\n",
    "            [ 1   2   7  14]\n",
    "            [ 4   5   6  13]\n",
    "            [ 9  10  11  12]\n",
    "    -Capa n:\n",
    "            [ 0      3      8     15    .  (n^2)+2n  ]\n",
    "            [ 1      2      7     14    .  (n^2)+2n-1]\n",
    "            [ 4      5      6     13    .  (n^2)+2n-2]\n",
    "            [ 9     10     11     12    .  (n^2)+2n-3]\n",
    "              .      .      .      .    .      .\n",
    "            [n^2 (n^2)+1 (n^2)+2 (n^2)+3.    (n^2)+n ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para su simplificación, hemos decidido cambiar la orientación propuesta en el enunciado del problema. Los resultados obtenidos en la matriz Q no van a variar de ninguna forma, lo único es que la matriz obtenida estaría desplazada __90º__ respecto de la original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modular el código hemos separado la función que ejecuta el bucle __n__ veces (n = número de capas que queramos en la matriz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrix(m,tam):\n",
    "    layer=0\n",
    "    for i in range(tam):\n",
    "        [m,layer]=addLayerToMatrix(m,i,layer)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la función que añade una capa más a la matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLayerToMatrix(matrix, layer, count):\n",
    "    for i in range(layer):\n",
    "        \n",
    "        matrix[layer][i] = int(count)\n",
    "        count=count+1\n",
    "        \n",
    "    for i in range(layer+1):\n",
    "        \n",
    "        matrix[layer-i][layer] = int(count)\n",
    "        count=count+1\n",
    "    \n",
    "    return [matrix, count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De ahí procedemos a crear el tablero deseado según el valor que hayamos introducido al principio del documento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   3   8  15  24  35  48  63  80  99 120 143 168 195 224 255]\n",
      " [  1   2   7  14  23  34  47  62  79  98 119 142 167 194 223 254]\n",
      " [  4   5   6  13  22  33  46  61  78  97 118 141 166 193 222 253]\n",
      " [  9  10  11  12  21  32  45  60  77  96 117 140 165 192 221 252]\n",
      " [ 16  17  18  19  20  31  44  59  76  95 116 139 164 191 220 251]\n",
      " [ 25  26  27  28  29  30  43  58  75  94 115 138 163 190 219 250]\n",
      " [ 36  37  38  39  40  41  42  57  74  93 114 137 162 189 218 249]\n",
      " [ 49  50  51  52  53  54  55  56  73  92 113 136 161 188 217 248]\n",
      " [ 64  65  66  67  68  69  70  71  72  91 112 135 160 187 216 247]\n",
      " [ 81  82  83  84  85  86  87  88  89  90 111 134 159 186 215 246]\n",
      " [100 101 102 103 104 105 106 107 108 109 110 133 158 185 214 245]\n",
      " [121 122 123 124 125 126 127 128 129 130 131 132 157 184 213 244]\n",
      " [144 145 146 147 148 149 150 151 152 153 154 155 156 183 212 243]\n",
      " [169 170 171 172 173 174 175 176 177 178 179 180 181 182 211 242]\n",
      " [196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 241]\n",
      " [225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240]]\n"
     ]
    }
   ],
   "source": [
    "zeroMatrix= np.zeros((num_fil,num_col))\n",
    "tablero = createMatrix(zeroMatrix,size).astype(int)\n",
    "print(tablero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de la matriz R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado el tablero procederemos al calculo de la matriz R. Para ello nos basaremos en las posiciones de la matriz tablero. En esta ocasión los únicos movimientos válidos son los movimientos perpendiculares y diagonales. \n",
    "\n",
    "Para que la matriz obtenida no tenga en cuenta los espacios fuera del tablero, tenemos que controlar los movimientos en los bordes del tablero. Dicho de otra forma controlamos cuando cualquiera de los índices del bucle se encuentra en la posición 0 o en la posición máxima menos 1.\n",
    "\n",
    "Cuando los índices se encuentren entre los valores mencionados en el párrafo anterior no existe restricción, menos que no puede ir a sí mismo. Esto último lo hemos decidido para que el entrenamiento de la matriz Q se ejecute de la forma más rápido posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este método obtiene automáticamente la Matriz R a partir del tablero (m) introducido por el usuario\n",
    "def obtainRMatrix(m):\n",
    "    Rsize = size**2\n",
    "    rMatrix = inicializeRMatrix(num_col,num_fil)\n",
    "    res=[]\n",
    "    for i in range(num_fil):\n",
    "        for j in range(num_col):\n",
    "            n0 = m[i][j]\n",
    "            if 0<i<num_fil-1:\n",
    "                rMatrix[m[i][j]][m[i-1][j]]=0\n",
    "                rMatrix[m[i][j]][m[i+1][j]]=0\n",
    "                \n",
    "                if(0<j<num_col-1):\n",
    "                    rMatrix[m[i][j]][m[i-1][j-1]]=0\n",
    "                    rMatrix[m[i][j]][m[i][j-1]]=0\n",
    "                    rMatrix[m[i][j]][m[i-1][j+1]]=0\n",
    "                    rMatrix[m[i][j]][m[i+1][j-1]]=0\n",
    "                    rMatrix[m[i][j]][m[i+1][j+1]]=0\n",
    "                    rMatrix[m[i][j]][m[i][j+1]]=0\n",
    "                    \n",
    "                if(j==0):\n",
    "                    rMatrix[m[i][j]][m[i-1][j+1]]=0\n",
    "                    rMatrix[m[i][j]][m[i][j+1]]=0\n",
    "                    rMatrix[m[i][j]][m[i+1][j+1]]=0\n",
    "                    \n",
    "                if(j==num_col-1):\n",
    "                    rMatrix[m[i][j]][m[i-1][j-1]]=0\n",
    "                    rMatrix[m[i][j]][m[i][j-1]]=0\n",
    "                    rMatrix[m[i][j]][m[i+1][j-1]]=0  \n",
    "                    \n",
    "            elif(i==0):\n",
    "                rMatrix[m[i][j]][m[i+1][j]]=0             \n",
    "                \n",
    "                if(j==0):\n",
    "                    rMatrix[m[i][j]][m[i+1][j+1]]=0             \n",
    "                    rMatrix[m[i][j]][m[i][j+1]]=0   \n",
    "                    \n",
    "                if(j==num_col-1):\n",
    "                    rMatrix[m[i][j]][m[i+1][j-1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i][j-1]]=0 \n",
    "                    \n",
    "                if(0<j<num_col-1):\n",
    "                    rMatrix[m[i][j]][m[i+1][j-1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i+1][j+1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i][j+1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i][j-1]]=0  \n",
    "                    \n",
    "            elif i==num_fil-1:\n",
    "                rMatrix[m[i][j]][m[i-1][j]]=0  \n",
    "                if(j==0):\n",
    "                    rMatrix[m[i][j]][m[i-1][j+1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i][j+1]]=0  \n",
    "                if(j==num_col-1):\n",
    "                    rMatrix[m[i][j]][m[i-1][j-1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i][j-1]]=0  \n",
    "                if(0<j<num_col-1):\n",
    "                    rMatrix[m[i][j]][m[i-1][j+1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i-1][j-1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i][j+1]]=0  \n",
    "                    rMatrix[m[i][j]][m[i][j-1]]=0  \n",
    "    return rMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada la función necesitamos añadirle el estado objetivo. Para ello hemos creado la funcion addOjetives. Aunque en este caso únicamente le añadimos sólo un objetivo, hemos implementado la opción de que se le puedan añadir varios objetivos.\n",
    "\n",
    "La forma de hacerlo es simple. Todos los estados que lleven hacia los estados objetivo tendrán una recompensa mayor que un estado intermedio. Para ello tenemos que modificar los elementos de la columna del estado objetivo en la matriz R. Específicamente cambiaremos los elementos que se correspondan con acciones posibles (recompensa 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addObjetives(R,objetives):\n",
    "    for objetive in objetives:\n",
    "        for i in range(num_fil**2):\n",
    "            R[i][objetive] = 100 if (R[i][objetive]==0) else -1       \n",
    "        R[objetive][objetive] = 100\n",
    "    return R          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así obtenemos la matriz R final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "rMatrix=obtainRMatrix(tablero)\n",
    "rMatrix=addObjetives(rMatrix,[objetivo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de aprendizaje \n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este apartado es el núcleo de nuestro proyeco. El momento en el que el algoritmo aprende y optimiza el camino hacia el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello, primero inicializamos la matriz __Q__ con todos sus valores a 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "qMatrix = np.zeros((size**2,size**2)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente prodecemos al entrenamiento de la matriz __Q__. Para ello, como podemos ver en el método __entrenarQ__. De entrada necesitamos los elementos anteriormente calculados: matriz R, matriz Q, además de el número Gamma, el objetivo (obtenido al principio del documento), y el número de episodios que queremos que el algoritmo ejecute de entrenamiento. \n",
    "\n",
    "Un __episodio abarca__ desde que el agente se encuentra en el __estado inicial__ hasta que encuentra el __estado objetivo__.\n",
    "Por lo tanto, nos es dificil pensar que cuantos más episodios introduzcamos, mejor se va a entrenar el algoritmo. Esta asunción no esta alejada de la realidad, pero no tiene en cuenta que pasados un número de episodios la matriz __Q__ no se modifica, por lo tanto el algoritmo no mejora el aprendimiento. Esto nos lleva a pensar que este aprendimiento es asintótico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenarQ ( Q, R, Gamma, objetivo, num_episodios ):\n",
    "    while num_episodios > 0:\n",
    "        estado = random.randint ( 0, num_fil**2 - 1 )\n",
    "        while ( estado != objetivo ):\n",
    "            #acciones = acciones ( R, estado )\n",
    "            accion_elegida = random.choice ( acciones ( R, estado ) )\n",
    "            siguiente_estado = accion_elegida\n",
    "            Q [ estado, accion_elegida ] = R [ estado, accion_elegida ] + Gamma * maxQValue(Q,R,siguiente_estado)\n",
    "            estado = siguiente_estado\n",
    "        num_episodios = num_episodios - 1\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la ejecución del método __entrenarQ()__ hacemos uso de dos métodos diferentes: __maxValue()__ y __acciones()__. El primero obtiene el valor máximo dentro de las acciones posibles dentro de un estado, y el segundo obtiene las acciones posibles dentro de un estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxQValue(Q, R, estado):\n",
    "    #obtener los estados accesibles desde la matriz R\n",
    "    rRow = R[estado]\n",
    "    qRow = Q[estado]\n",
    "    possibleStatesIndex = np.where(rRow != -1)\n",
    "    #obtener los valores en la matriz Q\n",
    "    qLista=qRow[possibleStatesIndex]\n",
    "    #devolver el máximo\n",
    "    maxValue=np.amax(qLista)\n",
    "    return maxValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acciones ( matriz, estado ):\n",
    "    fila_estado = matriz [ estado , : ]\n",
    "    acciones = [ ]\n",
    "    for n in range ( len ( fila_estado ) ):\n",
    "        if (fila_estado [ n ] != -1):\n",
    "            acciones.append(n)\n",
    "    return (acciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a entrenar la matriz __Q__: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-a566a1a55359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mQq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentrenarQ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjetivo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-196-101f021d403d>\u001b[0m in \u001b[0;36mentrenarQ\u001b[1;34m(Q, R, Gamma, objetivo, num_episodios)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mestado\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mobjetivo\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;31m#acciones = acciones ( R, estado )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0maccion_elegida\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0macciones\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestado\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0msiguiente_estado\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccion_elegida\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mQ\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mestado\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccion_elegida\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mestado\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccion_elegida\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mGamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmaxQValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msiguiente_estado\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-201-89c2c6e2f378>\u001b[0m in \u001b[0;36macciones\u001b[1;34m(matriz, estado)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0macciones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mlen\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mfila_estado\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfila_estado\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0macciones\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0macciones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Qq = entrenarQ(qMatrix, rMatrix, 0.8, objetivo, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Normalizamos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qMatrix=(qMatrix/np.max(qMatrix))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo así la matriz __Q__ entrenada y normalizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camino eficiente\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Último paso será pues, obtener el camino más eficiente hacia el __estado objetivo__ siendo una lista con los estados que llevan desde el __estado inicial__ hasta el objetivo.\n",
    "\n",
    "Para su ejecución simplemente tendríamos que observar cuál es la acción que nos ofrece una mayor recompensa para cada estado, buscando en la matriz Q. Incluyendo así los estados visitados en una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOptimalWay(initialState, Q, objetive):\n",
    "    #paso1 \n",
    "    actualState = initialState\n",
    "    camino = [initialState]\n",
    "    contador=0\n",
    "    totalValue=0\n",
    "    #paso2\n",
    "    while(actualState!=objetive):\n",
    "        qRow = Q[actualState]\n",
    "        maxValue = np.max(qRow)\n",
    "        maxValueIndex = np.where(qRow == maxValue)\n",
    "        nextState=maxValueIndex[0][0]\n",
    "        camino.append(nextState)\n",
    "        #paso3\n",
    "        totalValue += maxValue \n",
    "        actualState=nextState\n",
    "        print(actualState)\n",
    "        contador=contador+1\n",
    "    return camino, contador, totalValue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "[camino,contador,totalValue]=findOptimalWay(8, qMatrix, objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
